[
["index.html", "Calibration - Prediction: a tutorial Preamble", " Calibration - Prediction: a tutorial Pascal Pernot, Michèle Désenfant and François Hennebelle 2016-12-22 Preamble TBD "],
["intro.html", "Chapitre 1 Introduction", " Chapitre 1 Introduction "],
["stat-model.html", "Chapitre 2 Statistical modeling of calibration data 2.1 Calibration models", " Chapitre 2 Statistical modeling of calibration data 2.1 Calibration models One considers datasets of the type \\(D=\\{x_i,y_i\\}_{i=1}^N\\) where \\(x\\) is the control variable, and \\(y\\) the dependent variable. A calibration model \\(m(.;\\theta)\\) with parameters \\(\\theta\\) links these variables \\[\\begin{equation} y_i = m(x_i;\\theta) + \\epsilon_i, \\tag{2.1} \\end{equation}\\] where \\(\\epsilon_i\\) is an additive noise variable to be defined according to the available information on data uncertainty. 2.1.1 Random errors In many cases, one assumes that random errors are described by a normal distribution centered at zero \\[\\begin{equation} \\epsilon_i \\sim \\mathcal{N}(0,\\sigma_i), \\tag{2.2} \\end{equation}\\] where \\(\\sigma_i\\) is the standard deviation of measurements at \\(x_i\\), which might be constant. 2.1.2 Systematic errors The GUM (BIPM et al. 2008) recommends to incorporate systematic errors in the measurement model. What remains in the uncertainty model is the uncertainty on such corrections. However, the measurement uncertainties published in the scientific or technical literature often result from the combination of random and systematic contributions. In such cases, a variance-covariance matrix (\\(\\boldsymbol{\\Sigma}_y\\)) should ideally be provided with the data (BIPM et al. 2011). The error variables are no longer statistically independent and should be described as a \\(N\\)-vector \\[\\begin{equation} \\boldsymbol{\\epsilon} \\sim \\mathcal{N}_N(0,\\boldsymbol{\\Sigma}_y), \\tag{2.3} \\end{equation}\\] where \\(\\mathcal{N}_N(.,.)\\) is a \\(N\\)-variate normal distribution. Unfortunately, it is rarely the case that \\(\\boldsymbol{\\Sigma}_y\\) is available, or when available it might be corrupted by excessive rounding. The modeler has therefore often to build \\(\\boldsymbol{\\Sigma}_y\\) from limited information, and/or to do some reverse uncertainty engineering, i.e. to infer the respective contributions of both error types. Note that this is possible only in a limited number of cases, and with strong hypotheses on the contributions. Considering that two error sources have been aggregated into a single one with standard deviation \\(u_{tot}\\), one redefines the calibration model as \\[\\begin{equation} y_i = m(x_i;\\theta) + \\epsilon_{i,r} + \\epsilon_s, \\tag{2.4} \\end{equation}\\] where \\(\\epsilon_{\\{r,s\\}} \\sim N(0,u^2_{\\{r,s\\}})\\). Both \\(u_r\\) et \\(u_s\\) are unknown with constraint \\(u_r^2 + u_s^2 = u_{tot}^2\\). The covariance of two measurements is \\[\\begin{equation} {\\rm Cov}(y_i,y_j) = u_r^2\\ \\delta_{ij} + u_s^2, \\tag{2.5} \\end{equation}\\] and one can also express the covariance matrix using a correlation coefficient \\(\\rho\\) \\[\\begin{equation} \\boldsymbol{\\Sigma}_y=u^{2}_{tot}\\left(\\begin{array}{cccc} 1 &amp; \\rho &amp; \\cdots &amp; \\rho\\\\ \\rho &amp; 1 &amp; \\ddots &amp; \\vdots\\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; \\rho\\\\ \\rho &amp; \\cdots &amp; \\rho &amp; 1 \\end{array}\\right), \\ {\\rm with}\\ \\rho=\\frac{u^2_{s}}{u^2_{tot}} \\tag{2.6} \\end{equation}\\] References "],
["methods.html", "Chapitre 3 Calibration Methods 3.1 Estimation of the mean and standard deviation of a sample 3.2 Linear calibration/prediction 3.3 Prediction 3.4 \\(\\chi^2\\) analysis", " Chapitre 3 Calibration Methods We describe our methods in this chapter. Cite (Yardin 2013) 3.1 Estimation of the mean and standard deviation of a sample \\[ \\begin{aligned} y_i &amp;= \\mu + \\epsilon_i \\\\ \\epsilon_i &amp;\\sim N(0,\\sigma) \\end{aligned} \\] Likelihood \\[ p(D|\\mu,\\sigma) = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^N} \\exp\\left(-\\sum_i\\frac{(y_i-\\mu)^2} {2\\sigma^2}\\right) \\] Prior \\[ p(\\mu,\\sigma) \\propto \\sigma^{-1} \\] 3.1.1 Estimation of \\(\\mu\\) \\[ \\begin{aligned} p(\\mu|D) &amp;= \\int p(\\mu,\\sigma|D) d\\sigma \\\\ &amp;= \\int \\frac{1}{\\sqrt{2\\pi}^N\\sigma^{N+1}} \\exp\\left(-\\sum_i\\frac{(y_i-\\mu)^2} {2\\sigma^2}\\right)\\ d\\sigma \\\\ &amp;\\propto \\left(1 + \\frac{(\\bar{y}-\\mu)^2}{s^2}\\right)^{-N/2} \\\\ {\\rm avec}\\ \\bar{y} &amp;= \\frac{1}{N-1}\\sum_i y_i\\ \\ {\\rm et}\\ \\ s^2=\\frac{1}{N-1}\\sum_i (y_i - \\bar{y})^2 \\end{aligned} \\] which is a Student’s distribution with \\(N-1\\) degrees of freedom. One used \\(\\int_{0}^{\\infty}dx\\,x^{-n}\\exp\\left(-a/x^{2}\\right)\\propto a^{-(n-1)/2}\\) \\(\\sum_{i=1}^{N}\\left(y_{i}-\\mu\\right)^{2}=N\\left(s^{2}+\\left(\\overline{y}-\\mu\\right)^{2}\\right)\\) \\[ \\begin{aligned} \\rm{E}(\\mu) &amp;= \\bar{y} \\\\ \\rm{Var}(\\mu) &amp;= \\frac{N-1}{N-3}\\frac{s^2}{N} \\end{aligned} \\] 3.1.2 Estimation of \\(\\sigma\\) \\[ \\begin{aligned} p(\\sigma|D)&amp;=\\int_{-\\infty}^{\\infty}d\\mu\\,p(\\mu,\\sigma|D)\\\\ &amp;\\propto\\frac{1}{\\sigma^{N+1}}\\exp\\left(-\\frac{Ns^{2}}{2\\sigma^{2}}\\right)\\int_{-\\infty}^{\\infty}d\\mu\\exp\\left(-\\frac{N\\left(\\overline{y}-\\mu\\right)^{2}}{2\\sigma^{2}}\\right) \\end{aligned} \\] Using \\[ \\int_0^\\infty dx\\,x^{-n}e^{-a/x^{2}}=\\frac{1}{2}\\Gamma\\left(\\frac{n-1}{2}\\right)a^{(1-n)/2} \\] one gets \\[ p(\\sigma|D)\\propto\\frac{1}{\\sigma^{N}}\\exp\\left(-\\frac{Ns^{2}}{2\\sigma^{2}}\\right) \\] which is an Inverse Gamma distribution. \\[ \\begin{align*} &lt;\\sigma&gt; &amp; =s\\,\\sqrt{\\frac{N}{2}}\\frac{\\Gamma\\left[(N-2)/2\\right]}{\\Gamma\\left[(N-1)/2\\right]};\\,&lt;\\sigma^{2}&gt;=\\frac{N}{N-3}s^{2}\\\\ u_{\\sigma} &amp; =s\\,\\sqrt{\\frac{N}{N-3}-\\frac{N}{2}\\left(\\frac{\\Gamma\\left[(N-2)/2\\right]}{\\Gamma\\left[(N-1)/2\\right]}\\right)^{2}} \\end{align*} \\] 3.1.3 Prediction \\[ \\begin{aligned} p(y^*|D)&amp;=\\int_{-\\infty}^{\\infty}d\\mu\\int_{0}^{\\infty}d\\sigma\\:p(y^*|\\mu,\\sigma)\\:p(\\mu,\\sigma|D)\\\\ &amp;\\propto\\left(1+\\frac{1}{N+1}\\left(\\frac{y^*-\\overline{y}}{s}\\right)^{2}\\right)^{-N/2} \\end{aligned} \\] Using Student’s distribution properties, one gets \\[ y^*=\\overline{y}\\pm\\sqrt{\\frac{N+1}{N-3}}s \\] If \\(\\sigma\\) is known beforehand, prediction is a little less uncertain \\[ y^*=\\overline{y}\\pm\\sqrt{\\frac{N+1}{N}}\\sigma \\] 3.2 Linear calibration/prediction 3.2.1 Moindres carrés pondérés Propagation des incertitudes lors de l’estimation d’une droite d’étalonnage et de son utilisation en prédiction On dispose de \\(N\\) triplets de valeurs \\(\\{x_i,\\,y_i,\\,u_{y_i}\\}\\) (on suppose \\(u_{x_i}\\) négligeable ou nul) Modèle de mesure: \\[ y_{i}=a+b*x_{i}+\\epsilon_{i} \\] avec \\(\\epsilon_{i}\\sim N(\\mu=0,\\sigma=u_{y_{i}})\\) 0.5cm Régression linéaire par la méthode des “moindres carrés pondérés” \\[ (\\hat{a},\\hat{b})=\\mathrm{argmin}_{a,b}\\chi^{2}(a,b) \\] \\[ \\chi^{2}(a,b) = \\sum_{i=1}^{N}\\frac{\\left(y_{i}-a-bx_{i}\\right)^{2}} {u_{y_{i}}^{2}} \\] 3.2.2 Estimation des paramètres Résolution calculer les moyennes pondérées: \\(\\overline{x}=\\frac{\\sum w_{i}^{2}x_{i}}{\\sum w_{i}^{2}}\\) et \\(\\overline{y}=\\frac{\\sum w_{i}^{2}y_{i}}{\\sum w_{i}^{2}}\\), avec les poids \\(w_{i}=1/u_{y_{i}}\\) recentrer les données: \\(\\widetilde{x}_{i}=x_{i}-\\overline{x}\\) et \\(\\widetilde{y}_{i}=y_{i}-\\overline{y}\\) alors \\[\\hat{b}=\\frac{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}\\widetilde{y}_{i}}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}=\\sum_{i}\\frac{w_{i}^{2}\\widetilde{x}_{i}}{\\sum_{j}w_{j}^{2}\\widetilde{x}_{j}^{2}}\\,y_{i}\\] et \\[\\hat{a}=\\overline{y}-\\hat{b}\\overline{x}\\] \\(\\hat{a}\\) et \\(\\hat{b}\\) sont corrélés 3.2.3 Variance/covariance des paramètres Le calcul des incertitudes-type sur les coefficients de régression peut être mené analytiquement (modèle linéaire en \\(y_{i}\\)) en appliquant le GUM: \\[u_{b}^{2} = \\sum_{i}\\left(\\frac{w_{i}^{2}\\widetilde{x}_{i}}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}\\right)^{2}\\,u_{y_{i}}^{2}=\\frac{1}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}\\] \\[u_{a}^{2} = \\mathrm{Var}(\\overline{y})+\\overline{x}^{2}u_{b}^{2}=\\frac{1}{\\sum w_{i}^{2}}+\\frac{\\overline{x}^{2}}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}\\] \\[\\mathrm{cov}(\\hat{a},\\hat{b}) = \\mathrm{cov}(\\overline{y},\\hat{b})-\\overline{x}*\\mathrm{cov}(\\hat{b},\\hat{b})=-\\frac{\\overline{x}}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}\\] 3.3 Prediction 3.3.1 Prédiction directe Connaissant les paramètres de la droite de régression et les covariances associées, on veut déterminer l’incertitude liée au calcul d’une valeur de \\(Y\\) en un point \\(x\\) quelconque à l’aide du modèle linéaire. Modèle statistique \\[Y=\\hat{a}+\\hat{b}X+\\epsilon\\] \\(\\hat{a}\\) et \\(\\hat{b}\\) représentent les paramètres incertains de la droite de régression; 0.5cm \\(X\\) représente l’ordonnée pour laquelle on veut une prédiction de \\(Y\\); \\(X\\) est éventuellement incertaine \\(x=x_{0}\\pm u_{x}\\); et 0.5cm \\(\\epsilon\\) représente une erreur aléatoire sur la mesure d’une valeur de \\(Y\\); \\(\\epsilon\\) est de moyenne nulle et d’amplitude dépendant éventuellement de \\(X\\), \\(\\sigma_{\\epsilon}(X)\\). On applique (encore) la loi de combinaison des variances (GUM): \\[y = \\hat{a}+\\hat{b}x_{0}+\\hat{\\epsilon}\\] \\[ \\begin{aligned} u_{y}^{2} &amp;= u_{a}^{2}+x_{0}^{2}u_{b}^{2}+2x_{0}\\mathrm{cov}(a,b)+\\hat{b}^{2}u_{x}^{2}+\\sigma_{\\epsilon}^{2}(x_{0})\\\\ &amp;= \\frac{1}{\\sum w_{i}^{2}}+\\frac{\\left(x_{0}-\\overline{x}\\right)^{2}}{\\sum_{i}w_{i}^{2}\\widetilde{x}_{i}^{2}}+\\hat{b}^{2}u_{x}^{2}+\\sigma_{\\epsilon}^{2}(x_{0}) \\end{aligned} \\] En remplaçant \\(w_{i}^{2}\\) par \\(1/\\sigma^{2}\\), et dans l’hypothèse d’une incertitude uniforme sur \\(Y\\) (\\(\\sigma_{\\epsilon}(X)\\equiv\\sigma\\)), on dérive l’expression pour la régression par moindres carrés “ordinaire” \\[u_{y}^{2} = \\frac{\\sigma^{2}}{N}+\\frac{\\sigma^{2}\\left(x_{0}-\\overline{x}\\right)^{2}}{\\sum_{i}\\widetilde{x}_{i}^{2}}+\\hat{b}^{2}u_{x}^{2}+\\sigma^{2}\\] Selon les scenarii, plusieurs simplifications sont possibles et on retrouve des expressions d’usage courant: \\(X\\) sans incertitude et on veut l’incertitude sur l’estimation de la valeur moyenne de \\(Y\\) («confiance») \\[u_{y} =\\sigma \\sqrt{\\frac{1}{N}+\\frac{\\left(x_{0}-\\overline{x}\\right)^{2}}{\\sum_{i}\\widetilde{x}_{i}^{2}}}\\] \\(X\\) sans incertitude et on veut l’incertitude sur l’estimation d’une valeur unique de \\(Y\\) («prediction») \\[u_{y} =\\sigma \\sqrt{1+\\frac{1}{N}+\\frac{\\left(x_{0}-\\overline{x}\\right)^{2}}{\\sum_{i}\\widetilde{x}_{i}^{2}}}\\] Autres exemples: Possolo, A. (2013), Five examples of assessment and expression of measurement uncertainty. Appl. Stochastic Models Bus. Ind. 29:1–18. doi: 10.1002/asmb.1947 3.3.2 Prédiction inverse Connaissant les paramètres de la droite de régression et les covariances associées, on veut déterminer l’incertitude liée au calcul d’une valeur de X en un point y quelconque à l’aide du modèle linéaire inverse. Modèle statistique (non-linéaire en \\(\\hat{b}\\)) \\[X=\\frac{Y-\\hat{a}}{\\hat{b}}\\] \\(\\hat{a}\\) et \\(\\hat{b}\\) représentent les paramètres incertains de la droite de régression; \\(Y\\) est typiquement incertain \\(y=y_{0}\\pm u_{y}\\) On applique la loi de combinaison des variances (GUM): \\[x = (y_{0}-\\hat{a})/\\hat{b}\\] \\[ \\begin{aligned} u_{x}^{2} &amp;= \\frac{1}{\\hat{b}^{2}}u_{a}^{2}+\\left(\\frac{y_{0}-\\hat{a}}{\\hat{b}^{2}}\\right)^{2}u_{b}^{2}+2\\left(\\frac{y_{0}-\\hat{a}}{\\hat{b}^{3}}\\right)\\mathrm{cov}(a,b)+\\frac{1}{\\hat{b}^{2}}u_{y}^{2} \\\\ &amp;= \\frac{1}{\\hat{b}^{2}}\\left(u_{a}^{2}+x^{2}u_{b}^{2}+2x\\mathrm{cov}(a,b)+u_{y}^{2}\\right) \\end{aligned} \\] En remplaçant \\(w_{i}^{2}\\) par \\(1/\\sigma^{2}\\), et dans l’hypothèse d’une incertitude uniforme sur \\(Y\\) (\\(u_{Y}\\equiv\\sigma\\)), on dérive l’expression standard \\[u_{x} =\\frac{\\sigma}{\\left|\\hat{b}\\right|} \\sqrt{1+\\frac{1}{N}+\\frac{\\left(x-\\overline{x}\\right)^{2}}{\\sum_{i}\\widetilde{x}_{i}^{2}}} \\] 3.4 \\(\\chi^2\\) analysis grViz(&quot; digraph rmarkdown { node [shape = circle] &#39;Chi^2&#39; -&gt; {&#39;&lt;&lt; ndf&#39; &#39;# ndf&#39; &#39;&gt;&gt; ndf&#39;} &#39;&lt;&lt; ndf&#39; -&gt; {&#39;Model \\n too complex&#39; &#39;Uncertainty\\n too large&#39;} &#39;# ndf&#39; -&gt; {&#39;Proceed\\n to prediction&#39;} &#39;&gt;&gt; ndf&#39; -&gt; {&#39;Model \\n too simple&#39; &#39;Uncertainty\\n too small&#39;} }&quot;) References "],
["presentation.html", "Chapitre 4 Presentation of results 4.1 Rounding of results 4.2 Rounding of covariance matrices", " Chapitre 4 Presentation of results 4.1 Rounding of results 4.2 Rounding of covariance matrices See examples and Eq. 11 in (Ezhela 2007) Proposition in GUM-Supp2 (BIPM et al. 2011) 3.21 NOTE 5 “When presenting numerical values of the off-diagonal elements of a correlation matrix, rounding to three places of decimals is often sufficient. However, if the correlation matrix is close to being singular, more decimal digits need to be retained in order to avoid numerical difficulties when using the correlation matrix as input to an uncertainty evaluation. The number of decimal digits to be retained depends on the nature of the subsequent calculation, but as a guide can be taken as the number of decimal digits needed to represent the smallest eigenvalue of the correlation matrix with two significant decimal digits. For a correlation matrix of dimension 2 × 2, the eigenvalues λmax and λmin are 1± |r|, the smaller, λmin , being 1 − |r|, where r is the off-diagonal element of the matrix. If a correlation matrix is known to be singular prior to rounding, rounding towards zero reduces the risk that the rounded matrix is not positive semi-definite.” References "],
["cases.html", "Chapitre 5 Cases study 5.1 Residuals analysis 5.2 Impact of parameters covariance on prediction 5.3 Impact of data covariance 5.4 Impact of outliers / robust regression", " Chapitre 5 Cases study 5.1 Residuals analysis In this example, one analyzes data issued from a quadratic model, first with a linear model and then with a quadratic model. This will enable us to see how to invalidate a calibration model from its \\(\\chi^2\\) statistics and residuals structure. 5.1.1 Synthetic data A set of 10 data points for \\(x\\in[1,10]\\) is generated from a reference model \\[\\begin{equation} m(x;a,b,c)=a+b*x+c*x^2, \\tag{5.1} \\end{equation}\\] with \\((a=1,b=2,c=3)\\), and normal random noise of mean \\(0\\) standard deviation \\(u_r=10\\) is added to each value (Fig. 5.1). 5.1.2 Linear regression A Weighted Linear Regression (Eq. XXX) is performed with uniform weights \\(w_i = 1/u^2_r\\). The best fit model is plotted against the data in Fig. 5.1. Figure 5.1: Linear fit of quadratic data 5.1.2.1 Model criticism The agreement between data and the best regression line is not perfect, and the value of \\(\\chi^2\\) is much larger than the number of degrees of freedom. A closer look at the \\(\\chi^2\\) and residuals analysis in Fig. 5.2 reveals that the value of \\(\\chi^2\\) is well outside the 95% confidence interval for the \\(\\chi^2\\) distribution with 8 degrees of freedom. Besides, one sees in the residuals plot that the dispersion of residuals is much larger than the stated measurement uncertainties. This is enough to invalidate the calibration model, however, based on these criteria, there are two possible problems: underestimated measurement uncertainties: one could recover a ‘valid’ \\(\\chi^2\\) by multiplying \\(u_r\\) by a factor 2.4; an invalid measurement model. Although the former reason can never be formally excluded, the observation of a marked structure in the residuals plot points to a problem with the measurement model, which has to be solved before considering the other issue. Figure 5.2: Linear fit of quadratic data - residuals analysis: (left) chi2 value; (right) residuals 5.1.3 Calibration with quadratic model Following our rejection of the linear model, the data are reanalyzed with the quadratic model (Eq. (5.1)). The best fit of the WLS procedure is plotted against the data in Fig. 5.3. Figure 5.3: Quadratic fit of quadratic data 5.1.3.1 Model criticism The \\(\\chi^2\\) value and residuals analysis do not show the defects observed for the linear model (Fig. 5.4). One cannot rule out the validity of the calibration model. It will be used next to make predictions. Figure 5.4: Linear fit of quadratic data - residuals analysis: (left) chi2 value; (right) residuals The best fit parameters, their standard uncertainties and their correlation matrix are summarized in the following table. coefs uncert correl.a correl.b correl.c a -8.125 11.762 1.000 -0.909 0.814 b 5.262 4.912 -0.909 1.000 -0.975 c 2.671 0.435 0.814 -0.975 1.000 Note that, although the parameters best fit values seem far from the reference values, notably for \\(a\\) and \\(b\\), the latter lie within their uncertainty intervals (Fig. 5.5). Figure 5.5: 95% confidence ellipsoid for quadratic fit parameters. The point is the reference value. 5.2 Impact of parameters covariance on prediction The correlation matrix of the quadratic fit parameters contains very strong correlation coefficients. One might expect an important contribution of the parameters covariances to the uncertainty budget. 5.2.1 Uncertainty propagation Linear uncertainty propagation is used to predict the model uncertainty at a new point \\(x=5.5\\). The corresponding uncertainty budget is presented in Tab. 5.1. Table 5.1: LUP Uncertainty budget for the quadratic model at \\(x=5.5\\) E(x) u(x) j(x) [j(x).u(x)]² a -8.125e+00 1.18e+01 1.00e+00 1.38e+02 b 5.262e+00 4.91e+00 5.50e+00 7.30e+02 c 2.671e+00 4.35e-01 3.03e+01 1.73e+02 Cov -1.02e+03 Y 1.016e+02 4.78e+00 &lt;– 2.29e+01 The covariances contribute as a negative term, which reduces the sum of the positive contributions by about 98%. The predicted uncertainty on \\(y\\) is \\(u_y=\\) 4.8. If one ignores the parameters covariances, this value rises to \\(u_y=\\) 32. 5.2.2 Model prediction uncertainty The Monte Carlo UP approach is now used to evaluate prediction intervals for a whole range of \\(x\\) values. Two samples are generated, with and without parameters correlations. Scatterplots are shown in Fig. 5.6. Figure 5.6: Scatterplots of MC samples for the parameters of a quadratic model, with (red) and without (blue) correlations. 5.2.2.1 Comparison of predictions The 95 % model prediction uncertainty intervals are estimated from both parameters samples and shown in Fig. 5.7. The strong overestimation of uncertainty is visible over the whole \\(x\\) range when parameters correlation is ignored. In contrast, when correlation is taken into account, the prediction uncertainty typically presents a minimum within the calibration data range, and expands outside of this range. A direct comparison is provided in Fig. 5.8, where the standard uncertainties for both cases are reported on the same plot. As the measurement errors are considered to be random, the model prediction uncertainty is smaller than the measurement uncertainty (\\(u_r\\)) in the calibration range. The prediction uncertainty of a new measurement \\(y^*\\), \\(u_{y^*}=\\sqrt{u_r^2+u_y^2}\\) would however be larger. Figure 5.7: Comparison of model prediction uncertainty with (left panel) and without (right panel) parameters covariance. Figure 5.8: Effect of parameters covariance on model prediction uncertainty. 5.3 Impact of data covariance This example shows how ignoring data covariance can impede calibration and prediction. 5.3.1 Synthetic data A set of 10 data points for \\(x\\in[1,10]\\) is generated from the reference model \\(m(x;a=1,b=2,c=3)=a+b*x+c*x^2\\), and normal random noise of standard deviation \\(u_r=10\\) is added (Fig. 5.9). However, the data are declared to have an uncertainty \\(u_{tot}=30\\). 5.3.2 Simple regression with quadratic calibration model reg3 = lm(y ~ 1 + x + I(x^2), weights=1/uy^2) plotReg(x,y,uy,reg3,xp) Figure 5.9: Linear fit of quadratic data 5.3.2.1 Model criticism The curve for the best fit parameters is in perfect agreement with the data (Fig. 5.9). The value of \\(\\chi^2\\) is much smaller than the number of degrees of freedom, which should lead us to reconsider the calibration model. In fact, it is just below the lower limit of the 95% confidence interval (Fig. 5.10). This small value might have two origins: the model is too complex, and one is adjusting noise variations (overfitting); the stated uncertainties are too large (which is the case here). The latter hypothesis might be confirmed by the inspection of residual errors: their dispersion is much smaller than the stated uncertainties. For purely random errors, they should be similar. Therefore, the statistical description of errors in the calibration model is invalid. Figure 5.10: Linear fit of quadratic data - residuals analysis: (left) chi2 value; (right) residuals 5.3.3 Taking covariance into account In absence of a description of the partition of \\(u_{tot}\\) into random and systematic contributions, one might attempt to infer it from the data, using Eq. (2.4)-(2.6). The calibration problem is now non-linear, and one cannot use standard least-squares procedures. In this example, one uses a Bayesian method. 5.3.3.1 Bayesian inference of the calibration model’s parameters The a posteriori pdf is written \\[ p(a,b,c,\\rho|\\boldsymbol{D},u_{tot}) \\propto (\\det \\boldsymbol{\\Sigma}_y)^{−1/2} \\exp\\left(-\\frac{1}{2}E^T.\\boldsymbol{\\Sigma}_y^{-1}.E\\right)\\ p(a,b,c,\\rho) \\] with \\(E_i=y_i-m(x_i;a,b,c)\\) and a uniform a priori pdf \\(p(a,b,c,\\rho)\\propto U(\\rho;0,1)\\). A sample of the posterior pdf of length 1000 is generated by running a Markov Chain with the No U-Turn Sampler (cf. Stan), an summarized in Tab. 5.2. pars = c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;rho&#39;) fit = stan(model_code = mod1, model_name = &#39;Cov&#39;, data = list(N =length(xm), x=x, y=y, uy=uy), pars = pars, iter = 2000, chains = 1, warmup = 1000, verbose=FALSE, refresh=0) ## ## Elapsed Time: 0.38 seconds (Warm-up) ## 0.21 seconds (Sampling) ## 0.59 seconds (Total) X1 = cbind(extract(fit,&#39;a&#39;)[[1]], extract(fit,&#39;b&#39;)[[1]], extract(fit,&#39;c&#39;)[[1]], extract(fit,&#39;rho&#39;)[[1]]) colnames(X1)= pars tt = data.frame(Estimate=summary(fit)$summary[1:4,&#39;mean&#39;], Std.Error=summary(fit)$summary[1:4,&#39;sd&#39;],Correl = cor(X1)) kable(tt,caption=&#39;Statistical summary of posterior pdf.&#39;,digits=3) Table 5.2: Statistical summary of posterior pdf. Estimate Std.Error Correl.a Correl.b Correl.c Correl.rho a -0.537 29.985 1.000 -0.385 0.337 -0.001 b 0.328 5.233 -0.385 1.000 -0.974 -0.009 c 3.155 0.466 0.337 -0.974 1.000 0.012 rho 0.864 0.107 -0.001 -0.009 0.012 1.000 rgumlib::SAPlot(X1) Figure 5.11: Scatterplot of the posterior sample. Note: The estimated value of the correlation coefficient \\(\\rho=\\) 0.86 \\(\\pm\\) 0.11 is compatible with the reference value (0.89) (Fig. 5.12), and captures correctly the dominant role of systematic errors. With only 10 data points, it is not surprising that its uncertainty is large. Figure 5.12: Posterior distribution of \\(\\rho\\) and reference value. 5.3.4 Comparison of parameters uncertainties Comparison of the parameters mean values and uncertainties(Tab. 5.3) shows that the data correlation matrix has a weak impact on the former (the differences are smaller than the uncertainties). By contrast, the impact on the uncertainties is strong, especially for \\(b\\) and \\(c\\). Table 5.3: Comparison of the estimated values and uncertainties chen igniring systematic errors (Indep) or taking them into account (Corr) Indep Corr a 2.34 -0.54 b 0.18 0.33 c 3.16 3.16 u(a) 35.28 29.98 u(b) 14.74 5.23 u(c) 1.31 0.47 5.3.5 Uncertainty propagation In order to recycle the sample generated by the Bayesian analysis, on performs UP by Monte Carlo. A sample of parameters for the “independent data” scenario is generated from the summary statistics of the WLS quadratic fit. 5.3.5.1 Parameters samples The scatterplot of the parameters samples illustrates clearly the difference in uncertainty and correlation (Fig. 5.13). Figure 5.13: Quadratic fit parameters samples, with(red) and without (blue) correlation. 5.3.5.2 Comparison of predictions One plots below the 95% prediction intervals using both Monte Carlo samples (Fig. 5.14). The profiles are quite different: in the “independent data” hypothesis, the dispersion of predictions is notably smaller than the error bars in the calibration range and widens markedly as one extrapolates; at the opposite, for the “correlated data” hypothesis, predictions uncertainty intervals cover most of the error bars in the calibration range (prevalence of systematic errors) and widens weakly out of this range. The prediction uncertainties for both scenarii are plotted below and compared to \\(u_r\\) and \\(u_{tot}\\) (Fig. 5.15). . Figure 5.14: Comparison of model prediction uncertainty with (left panel) and without (right panel) treatment of data covariance. Figure 5.15: Effect of data covariance on model prediction uncertainty. 5.4 Impact of outliers / robust regression 5.4.1 Synthetic data Figure 5.16: Data with outliers 5.4.2 Gaussian errors Measurement model (standard): \\[ y_{i}=a+b*x_{i}+\\epsilon_{i} \\] with \\(\\epsilon_{i}\\sim N(\\mu=0,\\sigma=u_{y_{i}})\\) mean sd 2.5% 97.5% a 1.367 0.230 0.910 1.792 b 0.810 0.039 0.737 0.888 5.4.3 Student-t errors Measurement model: \\[ y_{i}=a+b*x_{i}+\\epsilon_{i} \\] with \\(\\epsilon_{i}\\sim t(\\nu,\\mu=0,\\sigma=u_{y_{i}})\\) and \\[ t(x|\\nu,\\mu,\\sigma) = \\frac{\\Gamma((\\nu+1)/2)} {\\sqrt{\\nu\\pi}\\sigma\\Gamma(\\nu/2)} \\left( 1+\\frac{1}{\\nu}\\left( \\frac{x-\\mu}{\\sigma}\\right)^2 \\right)^{-(\\nu+1)/2} \\] mean sd 2.5% 97.5% a -0.117 0.266 -0.659 0.415 b 0.995 0.043 0.914 1.082 nu 1.704 0.588 0.808 3.104 5.4.4 Comparison Figure 5.17: Robust fit of linear data with outliers Figure 5.18: Comparison of posterior samples for standard and robust regression "],
["references.html", "References", " References "]
]
